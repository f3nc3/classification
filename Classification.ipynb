{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Binary Classification\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/ (1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x*(1-x)\n",
        "\n",
        "def mean_square_error_loss(y_true,y_pred):\n",
        "  return np.mean((y_true - y_pred)**2) # Use subtraction for MSE\n",
        "\n",
        "input = np.array([[0,0],[0,1],[1,0],[1,1]]) # Correct the input array format\n",
        "output = np.array([[0],[1],[1],[0]]) # Correct the output array format\n",
        "\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "\n",
        "weights_input_hidden = np.random.rand(input_size,hidden_size)\n",
        "bias_hidden = np.random.rand(hidden_size)\n",
        "weights_hidden_output = np.random.randn(hidden_size,output_size)\n",
        "bias_output = np.random.rand(output_size)\n",
        "\n",
        "learning_rate = 0.1\n",
        "epochs = 20000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  hidden_input = np.dot(input,weights_input_hidden)+bias_hidden\n",
        "  hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "  final_input = np.dot(hidden_output,weights_hidden_output)+bias_output\n",
        "  final_output = sigmoid(final_input)\n",
        "\n",
        "  loss = mean_square_error_loss(output,final_output)\n",
        "\n",
        "  error_output = final_output - output # Fix variable name 'outputs'\n",
        "  gradient_output = error_output*sigmoid_derivative(final_output)\n",
        "\n",
        "  error_hidden = np.dot(gradient_output,weights_hidden_output.T)\n",
        "  gradient_hidden = error_hidden*sigmoid_derivative(hidden_output)\n",
        "\n",
        "  weights_hidden_output -= learning_rate*np.dot(hidden_output.T,gradient_output)\n",
        "  bias_output -= learning_rate*np.sum(gradient_output,axis=0)\n",
        "  weights_input_hidden -= learning_rate*np.dot(input.T,gradient_hidden)\n",
        "  bias_hidden -= learning_rate*np.sum(gradient_hidden,axis=0)\n",
        "\n",
        "  if (epoch + 1)%2000 == 0: # Correct indentation\n",
        "    print(f\"Epoch:{epoch + 1},loss:{loss:.6f}\")\n",
        "\n",
        "results = [] # Fix variable name 'result'\n",
        "for input_pair in input: # Fix variable name 'inputs', iterate over 'input'\n",
        "  hidden_input = np.dot(input_pair,weights_input_hidden)+bias_hidden\n",
        "  hidden_output = sigmoid(hidden_input)\n",
        "  final_input = np.dot(hidden_output,weights_hidden_output)+bias_output\n",
        "  final_output = sigmoid(final_input)\n",
        "  results.append((input_pair,np.round(final_output[0],2))) # Append a tuple\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BP1Qj4n30c9",
        "outputId": "0eab01b8-4fa7-4600-9246-70eb31a13a84"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:2000,loss:0.188099\n",
            "Epoch:4000,loss:0.138928\n",
            "Epoch:6000,loss:0.131366\n",
            "Epoch:8000,loss:0.128966\n",
            "Epoch:10000,loss:0.127840\n",
            "Epoch:12000,loss:0.127197\n",
            "Epoch:14000,loss:0.126785\n",
            "Epoch:16000,loss:0.126500\n",
            "Epoch:18000,loss:0.126291\n",
            "Epoch:20000,loss:0.126131\n",
            "[(array([0, 0]), 0.03), (array([0, 1]), 0.5), (array([1, 0]), 0.97), (array([1, 1]), 0.5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiclass Classification\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([0.8, 0.6, 0.7])\n",
        "y = np.array([0, 1, 0])\n",
        "w1 = np.array([[0.2, 0.4, 0.1],\n",
        "              [0.5, 0.3, 0.2],\n",
        "              [0.3, 0.7, 0.8]])\n",
        "w2 = np.array([[0.6, 0.4, 0.5],\n",
        "              [0.1, 0.2, 0.3],\n",
        "              [0.3, 0.7, 0.2]])\n",
        "b1 = np.array([0.1, 0.2, 0.3])\n",
        "b2 = np.array([0.1, 0.2, 0.3])\n",
        "print(f\"\\nOld w1:\\n{w1}\")\n",
        "print(f\"\\nOld b1:\\n{b1}\")\n",
        "print(f\"\\nOld w2:\\n{w2}\")\n",
        "print(f\"\\nOld b2:\\n{b2}\")\n",
        "\n",
        "# alpha\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return 1*(x>0)\n",
        "\n",
        "def softmax(x):\n",
        "  e_x = np.exp(x - np.max(x))\n",
        "  return e_x / e_x.sum()\n",
        "\n",
        "# Loss = summation(y_true * log(y_pred))\n",
        "def lossFunction(y_true,y_pred):\n",
        "  return -np.sum(y_true*np.log(y_pred))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # Forward propagation\n",
        "\n",
        "  # h1 = w1(x)+b1\n",
        "  h1 = np.dot(x, w1)+b1\n",
        "  # a1 = relu(h1)\n",
        "  a1 = relu(h1)\n",
        "\n",
        "  # h2 = w2(a1)+b2\n",
        "  h2 = np.dot(a1, w2)+b2\n",
        "  # a2 = softmax(h2)\n",
        "  a2 = softmax(h2)\n",
        "\n",
        "  loss = lossFunction(y, a2)\n",
        "\n",
        "  # Error = (y_pred - y)\n",
        "  error = a2 - y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXl0ZHL836Dh",
        "outputId": "b9c3cdcc-f7c2-42bf-d850-a24ff5478a27"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Old w1:\n",
            "[[0.2 0.4 0.1]\n",
            " [0.5 0.3 0.2]\n",
            " [0.3 0.7 0.8]]\n",
            "\n",
            "Old b1:\n",
            "[0.1 0.2 0.3]\n",
            "\n",
            "Old w2:\n",
            "[[0.6 0.4 0.5]\n",
            " [0.1 0.2 0.3]\n",
            " [0.3 0.7 0.2]]\n",
            "\n",
            "Old b2:\n",
            "[0.1 0.2 0.3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  # Backward propagation\n",
        "\n",
        "  # d(L)/d(w2) = Error * a1\n",
        "  d_L_d_w2 = error.dot(a1.T)\n",
        "\n",
        "  # d(L)/d(b2) = Error\n",
        "  d_L_d_b2 = error\n",
        "\n",
        "  d_L_d_hidden = np.dot(error, w2.T)\n",
        "  d_hidden_d_input = relu_derivative(h1)\n",
        "\n",
        "  # d(L)/d(w1) = Error * w2 * h1 * x\n",
        "  d_L_d_w1 = np.dot(x.T, d_hidden_d_input * d_L_d_hidden)\n",
        "\n",
        "  # d(L)/d(b1) = Error * w2 * h1\n",
        "  d_L_d_b1 = d_hidden_d_input * d_L_d_hidden\n",
        "\n",
        "\n",
        "  # Update weights\n",
        "  w2 -= learning_rate * d_L_d_w2\n",
        "  b2 -= learning_rate * d_L_d_b2\n",
        "\n",
        "  w1 -= learning_rate *d_L_d_w1\n",
        "  b1 -= learning_rate * d_L_d_b1\n",
        "\n",
        "  if (epoch + 1) % 1000 == 0:\n",
        "    print(f\"Epoch:{epoch + 1}, Loss: {loss:.4f}\")\n",
        "\n",
        "\n",
        "# Print final outputs\n",
        "print(\"\\nFinal outputs (a2):\\n\", a2)\n",
        "print(\"Final Loss:\\n\", loss)\n",
        "\n",
        "#results = []\n",
        "# for input_pair in inputs:\n",
        "#           hidden_input = np.dot(input_pair, weights_input_hidden)+bias_hidden\n",
        "#           hidden_output = sigmoid(hidden_input)\n",
        "#           final_input = np.dot(hidden_output, weights_hidden_output)+bias_output\n",
        "#           final_output = sigmoid(final_input)\n",
        "#           results.append((input_pair,np.round(final_output[0],2)))\n",
        "# print(results)\n",
        "\n",
        "print(f\"\\nUpdated w1:\\n{w1}\")\n",
        "print(f\"\\nUpdated b1:\\n{b1}\")\n",
        "print(f\"\\nUpdated w2:\\n{w2}\")\n",
        "print(f\"\\nUpdated b2:\\n{b2}\")\n",
        "print(f\"\\nOutput (a2) after epoch {epoch + 1}:\\n{a2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgIwhE6f3-XD",
        "outputId": "d86b2875-06ab-4a22-b570-46731752ee76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1000, Loss: 0.8774\n",
            "\n",
            "Final outputs (a2):\n",
            " [0.25502746 0.41586939 0.32910315]\n",
            "Final Loss:\n",
            " 0.8773840448515435\n",
            "\n",
            "Updated w1:\n",
            "[[0.20115017 0.40115017 0.10115017]\n",
            " [0.50115017 0.30115017 0.20115017]\n",
            " [0.30115017 0.70115017 0.80115017]]\n",
            "\n",
            "Updated b1:\n",
            "[0.09916084 0.19992592 0.30266563]\n",
            "\n",
            "Updated w2:\n",
            "[[0.60149895 0.40149895 0.50149895]\n",
            " [0.10149895 0.20149895 0.30149895]\n",
            " [0.30149895 0.70149895 0.20149895]]\n",
            "\n",
            "Updated b2:\n",
            "[0.09744973 0.20584131 0.29670897]\n",
            "\n",
            "Output (a2) after epoch 1000:\n",
            "[0.25502746 0.41586939 0.32910315]\n"
          ]
        }
      ]
    }
  ]
}